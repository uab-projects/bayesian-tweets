import argparse
import ast
from .constants import *

# helping methods
def evalTF(string):
	"""
	Given a string, evaluates that string and returns True or False depending
	if the string is "true" or "false", and most-important, case insensitive

	@throws 	Exception if string is not evaluable
	@param 		string to check if true or false
	@return 	True if string is "true", false if it's "false" (case insensitive)
	"""
	return ast.literal_eval(string.title())

# Default parser
DEFAULT_PARSER = argparse.ArgumentParser(
	# prog = 'iec.py'
	# usage = (generated by default)
	description = """This software intends to classify data according to if the text in the data is positive or not. To do so, we'll use Twitter as source of data, with a big data source with training samples""",
	epilog = "<> with â™¥ in UAB by ccebrecos & davidlj95",
	add_help = True,
	allow_abbrev = True
)
DEFAULT_PARSER.add_argument("-v","--version",
	action="version",
	version="Bayesian Tweets v0.0"
)
DEFAULT_PARSER.add_argument("-l","--log-level",
	metavar="level",
	action="store",
	help="""specifies the level of events to log. Events upper from that level
	will be displayed. Default is %s"""%(LOG_DEFAULT),
	type=str,
	choices=LOGS,
	default=LOG_DEFAULT
)
# About the input
DEFAULT_PARSER.add_argument("data_file",
	action="store",
	nargs="?",
	help="the file containing data to process",
	type=str,
	default=FILE_IN_DEFAULT
)

DEFAULT_PARSER.add_argument("-n","--n-samples",
	metavar="samples_number",
	action="store",
	help="number of items from the data loaded to analyze (starting from the begining), useful for debugging. If %d, all data is taken (default is %d)"%(N_SAMPLES_ALL,N_SAMPLES_DEFAULT),
	type=int,
	default=N_SAMPLES_DEFAULT
)

DEFAULT_PARSER.add_argument("-d","--col-data",
	metavar="n",
	action="store",
	help="""number of the column where the data is stored. Default is %d"""%(COL_DATA_DEFAULT),
	type=int,
	default=COL_DATA_DEFAULT
)

DEFAULT_PARSER.add_argument("-t","--col-sentiment",
	metavar="n",
	action="store",
	help="""number of the column where the sentiment classification is stored. Default is %d"""%(COL_SENTIMENT_DEFAULT),
	type=int,
	default=COL_SENTIMENT_DEFAULT
)
# About the input details
DEFAULT_PARSER.add_argument("--show-pre-data",
	metavar="true|false",
	action="store",
	nargs="?",
	help="""enables or disables printing information about raw data read (%s
	by default)"""%("enabled" if SHOW_DATA_PRE_DEFAULT else "disabled"),
	type=evalTF,
	const=True,
	default=SHOW_DATA_PRE_DEFAULT
)
DEFAULT_PARSER.add_argument("--show-input-data",
	metavar="true|false",
	action="store",
	nargs="?",
	help="""enables or disables printing information about data read (%s
	by default)"""%("enabled" if SHOW_DATA_INPUT_DEFAULT else "disabled"),
	type=evalTF,
	const=True,
	default=SHOW_DATA_INPUT_DEFAULT
)
DEFAULT_PARSER.add_argument("--show-filter-data",
	metavar="true|false",
	action="store",
	nargs="?",
	help="""enables or disables printing information about data once filtered (%s
	by default)"""%("enabled" if SHOW_DATA_FILTER_DEFAULT else "disabled"),
	type=evalTF,
	const=True,
	default=SHOW_DATA_FILTER_DEFAULT
)
DEFAULT_PARSER.add_argument("--show-iter-data",
	metavar="true|false",
	action="store",
	nargs="?",
	help="""enables or disables printing information about data to be used as training and validation in each iteration (%s
	by default)"""%("enabled" if SHOW_DATA_ITER_DEFAULT else "disabled"),
	type=evalTF,
	const=True,
	default=SHOW_DATA_ITER_DEFAULT
)
DEFAULT_PARSER.add_argument("--show-data",
	metavar="true|false",
	action="store",
	nargs="?",
	help="""enables or disables printing information about data read and filtered (equals to --show-input-data and --show-filter-data at same time)(%s
	by default)"""%("enabled" if SHOW_DATA_DEFAULT else "disabled"),
	type=evalTF,
	const=True,
	default=SHOW_DATA_DEFAULT
)
# About splitters
DEFAULT_PARSER.add_argument("-s","--splitter",
	action="store",
	help="splitter method to use in order to generate from filtered data one or more training and validation sets. Default is %s"%(SPLITTER_DEFAULT),
	type=str,
	choices=SPLITTERS_NAMES,
	default=SPLITTER_DEFAULT
)
DEFAULT_PARSER.add_argument("-k",
	action="store",
	metavar="groups",
	help="number of groups to generate in cross-validation splitter. Default is %d"%SPLITTER_CV_K_DEFAULT,
	type=int,
	default=SPLITTER_CV_K_DEFAULT,
)
DEFAULT_PARSER.add_argument("-i","--iterations",
	action="store",
	metavar="n",
	help="Number of times to loop datasets, if you used a splitter that generates multiple training and validaton sets. By default, k iterations will be used. Default is %d"%(ITERATIONS_DEFAULT),
	type=int,
	default=ITERATIONS_DEFAULT
)

# About filters
DEFAULT_PARSER.add_argument("--filter-lexical",
	action="store_true",
	help="""enables all recommended lexical filters"""
)
DEFAULT_PARSER.add_argument("--filter-types",
	action="store_true",
	help="""enables all recommended type filters (remove rare chars and numbers)"""
)
DEFAULT_PARSER.add_argument("--filter-twitter",
	action="store_true",
	help="""enables all recommended twitter filters (removes Twitter argot like @mentions #hashtags and RTs)"""
)
# About learning
DEFAULT_PARSER.add_argument("-e","--estimates",
	action="store",
	metavar="n",
	help="number of samples to imaginate when a word has not been found in training data and has to be classified (therefore this imaginary sample will be added to words in training data too). If n=1, we are applying Laplace smoothing. Default is %d"%(ESTIMATES_DEFAULT),
	type=float,
	default=ESTIMATES_DEFAULT
)
